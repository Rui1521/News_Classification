{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from numpy import linalg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(articles):\n",
    "    length = len(articles)  \n",
    "    Word = {}\n",
    "    bigdata = []\n",
    "    for x in articles:\n",
    "        for y in set(x):\n",
    "            if y in Word:\n",
    "                Word[y] += 1\n",
    "            else:\n",
    "                Word[y] = 1\n",
    "                \n",
    "            \n",
    "    for x in articles: \n",
    "        dic = {}\n",
    "        count =  Counter(x)\n",
    "        for word in set(x):\n",
    "            \n",
    "            max_count = max(count.values())\n",
    "            \n",
    "            num = Word[word]\n",
    "            \n",
    "            idf = math.log(length/(1+num))\n",
    "            \n",
    "            tf = (0.5+(0.5*(count[word]/len(x))/max_count))\n",
    "            \n",
    "            dic[word] = idf*tf\n",
    "            \n",
    "        bigdata.append(dic)\n",
    "        \n",
    "    Data = pd.DataFrame(data=bigdata)\n",
    "    Data = pd.DataFrame.as_matrix(Data.fillna(0))\n",
    "    Data = np.matrix(Data)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../datas/bbc_preprocessed.json') as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "articles = [x[\"content\"] for x in data]\n",
    "Data = tfidf(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  4.49437056,   4.49321471,   4.49325704, ...,   4.49485093,\n",
       "          12.44217723,   4.49278261]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yconvert(y,num):\n",
    "    Y = np.zeros((len(y),num))\n",
    "    for i in range(len(y)):\n",
    "        Y[i,y[i]] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cate = [x[\"category\"] for x in data]\n",
    "map = {'Technology':0,'Entertainment & Arts':1,'Business':2,'Health':3,'Science & Environment':4}\n",
    "categ = [map[x] for x in cate]\n",
    "\n",
    "\n",
    "random.seed(123)\n",
    "sam = random.sample(range(Data.shape[0]), Data.shape[0]//2)\n",
    "\n",
    "train = Data[sam,]\n",
    "valid = np.delete(Data,sam,0)\n",
    "\n",
    "train_categ = [categ[i] for i in range(len(categ)) if i in sam]\n",
    "valid_categ = np.delete(categ, sam, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category = yconvert(categ,5)\n",
    "train_category = category[sam,]\n",
    "valid_category = np.delete(category,sam,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Softmax function\n",
    "def softmax(x):\n",
    "    sm = (np.exp(x)/np.sum(np.exp(x),axis = 1))\n",
    "    return np.matrix(sm)\n",
    "\n",
    "#Cross-entropy loss fucntion\n",
    "def cross_entropy(prob, y, lam,w):\n",
    "    loss = -np.sum(np.multiply(np.log(prob), y)) +  lam*np.sum(abs(w))\n",
    "    return loss\n",
    "\n",
    "#Batch gradient descent\n",
    "def batch_gradient(x, y, prob, lam, w):\n",
    "    grad = np.dot(x.T, (y - prob)) + lam*abs(w)\n",
    "    return grad\n",
    "\n",
    "#Stoch gradient descent\n",
    "def stoch_gradient(x, y, prob, lam, w):\n",
    "    ran = random.randint(1, x.shape[0])\n",
    "    grad = np.dot(x[ran:(ran+49)%x.shape[0],].T, (y - prob)[ran:(ran+49)%x.shape[0],]) + lam*w\n",
    "    return grad\n",
    "\n",
    "def main(x, y, lam, alpha, e, stoch):\n",
    "    w = np.zeros((x.shape[1],y.shape[1]))\n",
    "    w = np.matrix(w)\n",
    "    n = len(x[:,1])\n",
    "    \n",
    "    prob = softmax(x * w)\n",
    "    loss = -(1/n) * cross_entropy(prob, y, lam, w)\n",
    "    grad = -(1/n) * batch_gradient(x, y, prob, lam, w)\n",
    "    w = w + (alpha * grad)\n",
    "    j=0\n",
    "    \n",
    "    for i in range(1000):\n",
    "        prob = softmax(x * w)\n",
    "        \n",
    "        loss0 = loss\n",
    "        \n",
    "        loss = -(1/n) * cross_entropy(prob, y, lam, w)\n",
    "        \n",
    "        if stoch:\n",
    "            j= (j+1)%x.shape[0]\n",
    "            grad = -(1/50) * stoch_gradient(x, y, prob, lam, w)\n",
    "            \n",
    "        else:\n",
    "            grad = -(1/n) * batch_gradient(x, y, prob, lam, w)\n",
    "        \n",
    "        w = w - (alpha * grad)\n",
    "        #print((abs(loss0-loss)), 0.05)\n",
    "        if (abs(loss0-loss) < e):\n",
    "            break\n",
    "        \n",
    "    return w\n",
    "\n",
    "def prediction(x,w):\n",
    "    \n",
    "    probs = softmax(np.dot(x,w))\n",
    "    preds = np.argmax(probs, axis = 1)\n",
    "    return preds\n",
    "\n",
    "def rate(x,w,y):\n",
    "    pred = prediction(x,w)\n",
    "    return np.sum(pred.T == y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross-validation to choose lambda\n",
    "def choose_lam(lam, Data, category, fold):\n",
    "    best_lam = 0\n",
    "    best_rate = 0\n",
    "    datas = Data\n",
    "    for l in lam:\n",
    "        r = 0 \n",
    "        for k in range(1,(datas.shape[0]//fold)+1):\n",
    "            leave = range((k-1)*fold,(k-1)*fold+fold)\n",
    "            aaa = main(np.delete(datas,leave,0), np.delete(category,leave,0), l, 0.1, False, 0.1)\n",
    "            r = r + rate(datas[leave,], aaa, categ[(k-1)*1600:(k-1)*1600+1600])\n",
    "            print (r)\n",
    "        if r/(datas.shape[0]//fold) > best_rate:\n",
    "            best_rate = r/9\n",
    "            best_lam = l\n",
    "    return best_rate, best_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam = 0\n",
    "W = main(train, train_category, lam, 0.1, 0.01, False)\n",
    "r = rate(valid, W, valid_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose the number of principle components\n",
    "def choose_k(Data):\n",
    "    S= linalg.svd(Data, full_matrices=False, compute_uv= False) \n",
    "    summ = sum(S)\n",
    "    for k in range(1,len(S)):\n",
    "        if sum(S[:k])/summ > 0.99:\n",
    "            break\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=100, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components = 100)\n",
    "pca.fit(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdata = np.matrix(pca.transform(Data))\n",
    "newtrain = newdata[sam,]\n",
    "newvalid = np.delete(newdata,sam,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=3, gamma=100, kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC(kernel='linear',C=10, gamma=100, decision_function_shape='ovo')\n",
    "model.fit(newvalid, valid_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, ..., 2, 4, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.plot(pca.explained_variance_, linewidth=2)\n",
    "pred = model.predict(newtrain)\n",
    "np.sum(pred == train_categ)/len(train_categ)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
